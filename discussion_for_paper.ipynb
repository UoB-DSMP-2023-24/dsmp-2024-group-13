{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kmer_feature_matrix_pipeline_funcs import *\n",
    "from filtering_functions import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('/Users/joesouber/OneDrive - University of Bristol/MSc Data Science/data science mini project/dsmp-2024-group-13/vdjdb_full.txt', sep='\\t')\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "df_preprocessed = preprocess_data(df)\n",
    "df_preprocessed_species = filter_by_species(df_preprocessed)\n",
    "df_preprocessed_species_min_score = filter_by_minimum_score(df_preprocessed_species)\n",
    "df_filtered_epitope = filter_by_length_range(df_preprocessed_species_min_score, 'antigen.epitope')\n",
    "df_done = filter_by_mhc_class(df_preprocessed_species_min_score)\n",
    "# Assuming 'df' is your original DataFrame\n",
    "min_instances = 5 # Set the minimum number of instances required for inclusion\n",
    "filtered_df = filter_by_epitope_instances(df_done, label_col='antigen.epitope', min_instances=min_instances)\n",
    "\n",
    "print(f\"Original DataFrame had {len(df_done)} rows.\")\n",
    "print(f\"Filtered DataFrame has {len(filtered_df)} rows.\")\n",
    "print(f\"unique epitopes in the filtered DataFrame: {filtered_df['antigen.epitope'].value_counts()}\")\n",
    "print(f\"Number of unique epitopes in the filtered DataFrame: {filtered_df['antigen.epitope'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALPHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X, y, feature_names, kmer_count_dict, epitope_names, epitope_to_int = create_features_matrix(filtered_df, include_alpha=False, include_beta=True, alpha_col='cdr3.alpha', beta_col='cdr3.beta', label_col='antigen.epitope', k=3)\n",
    "classifier = GradientBoostingClassifier(subsample=0.8,random_state=10, n_estimators=200, learning_rate=0.01, max_features='sqrt', max_depth=7)\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "auc_dict1, acc_list, precision_list, recall_list, all_class_reports, all_conf_matrices, clf, misclassified_instances, misclassified_details, X_train, X_test = predict_auc(X, y, classifier, 5, epitope_names, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X, y, feature_names, kmer_count_dict, epitope_names, epitope_to_int = create_features_matrix(filtered_df, include_alpha=True, include_beta=False, alpha_col='cdr3.alpha', beta_col='cdr3.beta', label_col='antigen.epitope', k=3)\n",
    "classifier = GradientBoostingClassifier(subsample=0.8,random_state=10, n_estimators=200, learning_rate=0.01, max_features='sqrt', max_depth=7)\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "auc_dict2, acc_list, precision_list, recall_list, all_class_reports, all_conf_matrices, clf, misclassified_instances, misclassified_details, X_train, X_test = predict_auc(X, y, classifier, 5, epitope_names, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X, y, feature_names, kmer_count_dict, epitope_names, epitope_to_int = create_features_matrix(filtered_df, include_alpha=True, include_beta=True, alpha_col='cdr3.alpha', beta_col='cdr3.beta', label_col='antigen.epitope', k=3)\n",
    "classifier = GradientBoostingClassifier(subsample=0.8,random_state=10, n_estimators=200, learning_rate=0.01, max_features='sqrt', max_depth=7)\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "auc_dict3, acc_list, precision_list, recall_list, all_class_reports, all_conf_matrices, clf, misclassified_instances, misclassified_details, X_train, X_test = predict_auc(X, y, classifier, 5, epitope_names, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boxplots comparison of alpha, beta and alphabeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# where auc_dict 1 is alpha, aucdict2 is beta and aucdict3 is alpha+beta\n",
    "def extract_average_auc(auc_dict):\n",
    "    \"\"\" Extracts and calculates average AUC per fold from a given AUC dictionary. \"\"\"\n",
    "    return [np.mean(list(fold.values())) for fold in auc_dict.values()]\n",
    "\n",
    "\n",
    "# Assuming auc_dict, auc_dict2, auc_dict3 are defined elsewhere and imported here\n",
    "average_auc_alpha = extract_average_auc(auc_dict1)      # from alpha\n",
    "average_auc_beta = extract_average_auc(auc_dict2)      # from beta\n",
    "average_auc_alpha_beta = extract_average_auc(auc_dict3)  # from alpha+beta\n",
    "\n",
    "# Combine the results into a list of lists for plotting\n",
    "data = [average_auc_alpha, average_auc_beta, average_auc_alpha_beta]\n",
    "labels = ['Alpha', 'Beta', 'Alpha+Beta']\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=data)\n",
    "plt.xticks(ticks=range(3), labels=labels)\n",
    "plt.title('Comparison of Average AUC across Different Feature Matrix Methods')\n",
    "plt.xlabel('Method')\n",
    "plt.ylabel('Average AUC')\n",
    "plt.show()\n",
    "plt.savefig('/content/alpha_vs_beta_vs_alphabeta.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# top and bottom performing epitopes for each method (change auc_dict to specify between methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Extracting average AUC values\n",
    "average_auc_per_fold = [np.mean(list(fold.values())) for fold in auc_dict3.values()]\n",
    "\n",
    "# Calculating epitope-specific AUCs across folds\n",
    "epitope_aucs = {epi: [] for epi in range(len(epitope_names))}  # Initialize dictionary with epitope indices\n",
    "for fold in auc_dict3.values():\n",
    "    for epitope, auc in fold.items():\n",
    "        if isinstance(epitope, int):  # Filtering out 'micro' and 'macro'\n",
    "            epitope_aucs[epitope].append(auc)\n",
    "\n",
    "# Calculating average AUC for each epitope\n",
    "average_auc_per_epitope = {epi: np.mean(aucs) for epi, aucs in epitope_aucs.items()}\n",
    "sorted_epitopes = sorted(average_auc_per_epitope.items(), key=lambda item: item[1])\n",
    "\n",
    "# Extracting top 3 and bottom 3 epitopes, converting index to names\n",
    "top_3_epitopes = [(epitope_names[epi], auc) for epi, auc in sorted_epitopes[-3:]]\n",
    "bottom_3_epitopes = [(epitope_names[epi], auc) for epi, auc in sorted_epitopes[:3]]\n",
    "\n",
    "# Converting to a list for plotting using names\n",
    "top_3_values = [epitope_aucs[epi] for epi, _ in sorted_epitopes[-3:]]\n",
    "bottom_3_values = [epitope_aucs[epi] for epi, _ in sorted_epitopes[:3]]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(data=top_3_values)\n",
    "plt.title('Top 3 Performing Epitopes')\n",
    "plt.xticks(ticks=range(3), labels=[name for name, _ in top_3_epitopes], rotation=90)\n",
    "plt.xlabel('Epitopes')\n",
    "plt.ylabel('AUC')\n",
    "plt.savefig('/content/alpha_beta_epitopes_topperformance.png')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(data=bottom_3_values)\n",
    "plt.title('Bottom 3 Performing Epitopes ')\n",
    "plt.xticks(ticks=range(3), labels=[name for name, _ in bottom_3_epitopes], rotation=90)\n",
    "plt.xlabel('Epitopes')\n",
    "plt.ylabel('AUC')\n",
    "plt.savefig('/content/apha_beta_epitopes_bottomperformance.png')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# top feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(clf, feature_names, top_n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# positional frequencies of top importance kmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "    # Create a list of tuples (feature_name, importance)\n",
    "feature_importance = list(zip(feature_names, importances))\n",
    "\n",
    "    # Sort the feature importances by most important first\n",
    "feature_importance = sorted(feature_importance, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Taking the top n features\n",
    "top_features = feature_importance[:50]\n",
    "features, scores = zip(*top_features)\n",
    "kmer_data = pd.DataFrame(list(kmer_count_dict.items()), columns=['kmer_pos', 'count'])\n",
    "kmer_data['kmer'], kmer_data['position'] = zip(*kmer_data['kmer_pos'].apply(lambda x: (x.split('_')[0], int(x.split('_')[1]))))\n",
    "\n",
    "# Convert `top_features` to uppercase to match the case in `kmer_data`\n",
    "top_kmers_upper = [f\"{feat.split('_')[0].upper()}_{feat.split('_')[1]}\" for feat, importance in top_features]\n",
    "\n",
    "# Filter `kmer_data` to include only the top k-mers\n",
    "top_kmer_data = kmer_data[kmer_data['kmer_pos'].isin(top_kmers_upper)]\n",
    "\n",
    "# Create the pivot table for the heatmap\n",
    "if not top_kmer_data.empty:\n",
    "    top_kmer_pivot = top_kmer_data.pivot(index='kmer_pos', columns='position', values='count').fillna(0)\n",
    "\n",
    "    # Generate the heatmap with the top 50 k-mers\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    sns.set(font_scale=1)\n",
    "    sns.heatmap(top_kmer_pivot, annot=True, cmap='YlGnBu', fmt=\".0f\", linewidths=.5, cbar_kws={'shrink': 0.5})\n",
    "    plt.title('Frequency of Top 50 k-mers Across Positions')\n",
    "    plt.xlabel('Position in Sequence')\n",
    "    plt.ylabel('k-mer')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No matching k-mers found in the dataset. Please check the feature names and dataset.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# zoom in on specific kmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Define the specific k-mer to visualize, e.g., 'ssi'\n",
    "specific_kmer = 'LIF'\n",
    "\n",
    "# Filter for all positions of the specific k-mer\n",
    "specific_kmer_data = kmer_data[kmer_data['kmer_pos'].str.contains(f'^{specific_kmer}_', case=False, regex=True)]\n",
    "\n",
    "# Create the pivot table for the heatmap\n",
    "if not specific_kmer_data.empty:\n",
    "    # Extract k-mer and position\n",
    "    specific_kmer_data['position'] = specific_kmer_data['kmer_pos'].apply(lambda x: int(x.split('_')[1]))\n",
    "    \n",
    "    # Pivot by position\n",
    "    kmer_position_pivot = specific_kmer_data.pivot_table(index='kmer_pos', columns='position', values='count', aggfunc='sum').fillna(0)\n",
    "\n",
    "    # Generate the heatmap\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.set(font_scale=1)\n",
    "    sns.heatmap(kmer_position_pivot, annot=True, cmap='YlGnBu', fmt=\".0f\", linewidths=.5, cbar_kws={'shrink': 0.5})\n",
    "    plt.title(f'Frequency of {specific_kmer} Across Positions')\n",
    "    plt.xlabel('Position in Sequence')\n",
    "    plt.ylabel('K-mer and Position')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No matching positions found for the specified k-mer. Please check the feature names and dataset.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kmers in multiple positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Extract k-mer names from kmer_pos by removing the position suffix\n",
    "kmer_data['kmer'] = kmer_data['kmer_pos'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "# Group by the extracted k-mer names and count unique positions for each k-mer\n",
    "kmer_position_counts = kmer_data.groupby('kmer')['kmer_pos'].nunique()\n",
    "\n",
    "# Filter to find k-mers that appear in more than one position\n",
    "kmers_in_multiple_positions = kmer_position_counts[kmer_position_counts > 1]\n",
    "kmers_in_multiple_positions_sorted = kmers_in_multiple_positions.sort_values(ascending=False)\n",
    "# Print the result\n",
    "kmers_in_multiple_positions_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kmers in multiple positions that are also in top importance (can specify top importance value, e.g top 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Extract k-mer names from kmer_pos by removing the position suffix\n",
    "kmer_data['kmer'] = kmer_data['kmer_pos'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "# Group by the extracted k-mer names and count unique positions for each k-mer\n",
    "kmer_position_counts = kmer_data.groupby('kmer')['kmer_pos'].nunique()\n",
    "\n",
    "# Filter to find k-mers that appear in more than one position\n",
    "kmers_in_multiple_positions = kmer_position_counts[kmer_position_counts > 1]\n",
    "kmers_in_multiple_positions_sorted = kmers_in_multiple_positions.sort_values(ascending=False)\n",
    "# Print the result\n",
    "kmers_in_multiple_positions_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kmers in both top importance list and  top multiple positions list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting k-mer name from kmer_pos and aggregating total count for each k-mer\n",
    "kmer_data['kmer'] = kmer_data['kmer_pos'].apply(lambda x: x.split('_')[0])\n",
    "total_counts = kmer_data.groupby('kmer')['count'].sum().reset_index()\n",
    "\n",
    "# Convert kmer to uppercase to standardize (assuming most populous already in uppercase)\n",
    "total_counts['kmer'] = total_counts['kmer'].str.upper()\n",
    "most_populous_kmers = total_counts.sort_values(by='count', ascending=False).head(30)\n",
    "\n",
    "# Assuming `clf` is your trained classifier and `feature_names` are available\n",
    "importances = clf.feature_importances_  # Example feature importances, replace with actual\n",
    "#feature_names = ['ssi_0', 'agy_1', 'gyg_2', 'ygg_3', 'ggs_4', 'qgn_5', 'ssi_6', 'agy_7', 'gyg_8', 'ygg_9'] * 15\n",
    "\n",
    "# Create a list of tuples (feature_name, importance) and sort by importance\n",
    "feature_importance = list(zip(feature_names, importances))\n",
    "feature_importance_sorted = sorted(feature_importance, key=lambda x: x[1], reverse=True)\n",
    "top_features = feature_importance_sorted[:30]\n",
    "\n",
    "# Convert `top_features` to uppercase k-mer names without positions for comparison\n",
    "top_kmers_upper = [feat[0].split('_')[0].upper() for feat in top_features]\n",
    "\n",
    "# Find common k-mers\n",
    "common_kmers = set(most_populous_kmers['kmer']).intersection(top_kmers_upper)\n",
    "print(\"Common k-mers in both top populous and top important lists:\", common_kmers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tying it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib-venn\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn2\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate the plots\n",
    "\n",
    "## First, the Venn diagram\n",
    "plt.figure(figsize=(8, 8))\n",
    "venn_labels = {'10': len(most_populous_kmers) - len(common_kmers), \n",
    "               '01': len(top_kmers_upper) - len(common_kmers), \n",
    "               '11': len(common_kmers)}\n",
    "venn2(subsets=venn_labels, set_labels=('Most Populous K-mers', 'Top Important K-mers'))\n",
    "plt.title(\"Venn Diagram of K-mer Overlap\")\n",
    "plt.show()\n",
    "\n",
    "## Then, the bar plot for the most populous k-mers\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='kmer', y='count', data=most_populous_kmers.head(30), \n",
    "            palette=[\"red\" if kmer in common_kmers else \"blue\" for kmer in most_populous_kmers['kmer'].head(30)])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Top 30 Most Populous K-mers (Red indicates Top Importance)')\n",
    "plt.xlabel('K-mer')\n",
    "plt.ylabel('Total Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task is now to get out results from SETE and TCRdist, plus possibly langauge approach, so that we can output a combined final graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# AUC values for each method\n",
    "auc_no_pca = np.array([0.772, 0.741, 0.768, 0.807, 0.815])\n",
    "auc_pca = np.array([0.793, 0.774, 0.741, 0.751, 0.748])\n",
    "auc_umap = np.array([0.639, 0.581, 0.643, 0.677, 0.65])\n",
    "auc_sete = np.array([0.767,0.696,0.751,0.707])\n",
    "auc_tcrdist = np.array([0.761838, 0.749288,\t0.770169,\t0.762701, 0.76629\t])\n",
    "\n",
    "# Calculate averages\n",
    "avg_no_pca = np.mean(auc_no_pca)\n",
    "avg_pca = np.mean(auc_pca)\n",
    "avg_umap = np.mean(auc_umap)\n",
    "avg_sete = np.mean(auc_sete)\n",
    "avg_tcrdist = np.mean(auc_tcrdist)\n",
    "\n",
    "# Calculate percentage differences\n",
    "perc_diff_pca_no_pca = ((avg_pca - avg_no_pca) / avg_no_pca) * 100\n",
    "perc_diff_umap_no_pca = ((avg_umap - avg_no_pca) / avg_no_pca) * 100\n",
    "perc_diff_sete_no_pca = ((avg_sete - avg_no_pca) / avg_no_pca) * 100\n",
    "perc_diff_tcrdist_no_pca = ((avg_tcrdist - avg_no_pca) / avg_no_pca) * 100\n",
    "\n",
    "# Plotting\n",
    "data = [auc_no_pca, auc_pca, auc_umap, auc_sete,auc_tcrdist]\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(data, labels=['No PCA', 'PCA', 'UMAP', 'SETE','TCRdist3'])\n",
    "ax.set_ylabel('AUC Scores')\n",
    "ax.set_title('Comparison of AUC Scores')\n",
    "\n",
    "# Show average values on the plot\n",
    "#for i, avg in enumerate([avg_no_pca, avg_pca, avg_umap], start=1):\n",
    "    #ax.text(i, avg + 0.01, f'{avg:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "(avg_no_pca, avg_pca, avg_umap,avg_sete,avg_tcrdist), (perc_diff_pca_no_pca, perc_diff_umap_no_pca,perc_diff_sete_no_pca,perc_diff_tcrdist_no_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# also need to finish extension stuff and do epitope analysis stuff, can be found on final_outputs.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
