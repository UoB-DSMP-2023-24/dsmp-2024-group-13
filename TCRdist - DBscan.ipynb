{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48ada53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dg/1nsyrpm96v149hfhy4g6qc0h0000gn/T/ipykernel_1741/2213001380.py:6: DtypeWarning: Columns (30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(path2, sep='\\t')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Missing Values': cdr3.alpha         5559\n",
       " v.alpha            5585\n",
       " j.alpha            5740\n",
       " cdr3.beta           925\n",
       " v.beta              921\n",
       " d.beta             8374\n",
       " j.beta              996\n",
       " species               0\n",
       " mhc.a                 0\n",
       " mhc.b                 0\n",
       " antigen.gene         31\n",
       " antigen.epitope       0\n",
       " vdjdb.score           0\n",
       " mhc.class             0\n",
       " dtype: int64,\n",
       " 'Missing Values df_cleaned': cdr3.alpha            0\n",
       " v.alpha               0\n",
       " j.alpha             122\n",
       " cdr3.beta             0\n",
       " v.beta                0\n",
       " d.beta             2656\n",
       " j.beta               74\n",
       " species               0\n",
       " mhc.a                 0\n",
       " mhc.b                 0\n",
       " antigen.gene         31\n",
       " antigen.epitope       0\n",
       " vdjdb.score           0\n",
       " mhc.class             0\n",
       " dtype: int64}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "path2 =  \"/Users/tomelder/Downloads/vdjdb-2023-06-01/vdjdb_full.txt\"\n",
    "data = pd.read_csv(path2, sep='\\t')\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = df[(df['vdjdb.score'] == 2) | (df['vdjdb.score'] == 3) | (df['vdjdb.score'] == 4)  | (df['vdjdb.score'] == 1) ]\n",
    "\n",
    "relevant_columns = [\n",
    "    'cdr3.alpha', 'v.alpha', 'j.alpha', 'cdr3.beta', 'v.beta', 'd.beta', 'j.beta',\n",
    "    'species', 'mhc.a', 'mhc.b', 'antigen.gene','antigen.epitope', 'vdjdb.score', 'mhc.class'\n",
    "]\n",
    "# For the purpose of this example, we won't filter out lower-quality scores to avoid data imbalance,\n",
    "# but in practice, you may want to consider this based on the dataset's size and distribution.\n",
    "filtered_data = df[relevant_columns]\n",
    "# Displaying the first few rows of the filtered dataset to verify the selection\n",
    "filtered_data\n",
    "# Checking for missing values in crucial columns\n",
    "missing_values = filtered_data.isnull().sum()\n",
    "\n",
    "# Examining the number of unique values in categorical columns for potential encoding strategies\n",
    "unique_values = filtered_data.nunique()\n",
    "\n",
    "# Removing rows with missing cdr3 sequences\n",
    "df_cleaned = filtered_data.dropna(subset=['cdr3.alpha','cdr3.beta'])\n",
    "df_cleaned.reset_index(drop=True, inplace=True)\n",
    "\n",
    "missing_values_df_cleaned = df_cleaned.isnull().sum()\n",
    "report = {\n",
    "    \"Missing Values\": missing_values,\n",
    "    \"Missing Values df_cleaned\": missing_values_df_cleaned\n",
    "}\n",
    "\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e1e3593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter lower bound for cdr3.alpha length: 12\n",
      "Enter upper bound for cdr3.alpha length: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dg/1nsyrpm96v149hfhy4g6qc0h0000gn/T/ipykernel_1741/163780384.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_name + '.length'] = df[column_name].apply(len)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter lower bound for cdr3.beta length: 12\n",
      "Enter upper bound for cdr3.beta length: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dg/1nsyrpm96v149hfhy4g6qc0h0000gn/T/ipykernel_1741/163780384.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_name + '.length'] = df[column_name].apply(len)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter lower bound for antigen.epitope length: 0\n",
      "Enter upper bound for antigen.epitope length: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dg/1nsyrpm96v149hfhy4g6qc0h0000gn/T/ipykernel_1741/163780384.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_name + '.length'] = df[column_name].apply(len)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the minimum score (inclusive) for vdjdb.score: 1\n",
      "Enter the species to filter by (separated by commas): HomoSapiens\n"
     ]
    }
   ],
   "source": [
    "def filter_by_length_range(df, column_name):\n",
    "    \"\"\"\n",
    "    Asks the user for length bounds and filters the DataFrame to include rows where the length of\n",
    "    the specified column's sequence falls within the provided bounds.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The data frame to filter.\n",
    "    column_name (str): The name of the sequence column to check (e.g., 'cdr3.alpha').\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A DataFrame filtered by the specified length range.\n",
    "    \"\"\"\n",
    "    # Ask the user for length bounds\n",
    "    lower_bound = int(input(f\"Enter lower bound for {column_name} length: \"))\n",
    "    upper_bound = int(input(f\"Enter upper bound for {column_name} length: \"))\n",
    "    \n",
    "    # Calculate the sequence lengths\n",
    "    df[column_name + '.length'] = df[column_name].apply(len)\n",
    "    \n",
    "    # Filter based on the length range\n",
    "    return df[(df[column_name + '.length'] >= lower_bound) & (df[column_name + '.length'] <= upper_bound)]\n",
    "\n",
    "def filter_by_species(df):\n",
    "    \"\"\"\n",
    "    Asks the user for species to filter by and filters the DataFrame to include rows where the\n",
    "    species column matches any of the species provided.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The data frame to filter.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A DataFrame filtered by the specified species.\n",
    "    \"\"\"\n",
    "    # Ask the user for species to filter by\n",
    "    input_species = input(\"Enter the species to filter by (separated by commas): \")\n",
    "    species_to_filter = [species.strip() for species in input_species.split(',')]\n",
    "    \n",
    "    return df[df['species'].isin(species_to_filter)]\n",
    "\n",
    "\n",
    "def filter_by_minimum_score(df, column='vdjdb.score'):\n",
    "    \"\"\"\n",
    "    Filters the DataFrame based on a minimum score inputted by the user for a specified column.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The data frame to filter.\n",
    "    column (str): The name of the column to apply the filter on. Defaults to 'vdjdb.score'.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: A DataFrame filtered based on the user-specified minimum score.\n",
    "    \"\"\"\n",
    "    # Prompting user for minimum score\n",
    "    min_score = input(f\"Enter the minimum score (inclusive) for {column}: \")\n",
    "    \n",
    "    # Validating user input\n",
    "    try:\n",
    "        min_score = int(min_score)\n",
    "        if min_score < 0 or min_score > 3:\n",
    "            print(\"Score out of range. Please enter a value between 0 and 3.\")\n",
    "            return df\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter an integer value.\")\n",
    "        return df\n",
    "    \n",
    "    # Filtering the DataFrame\n",
    "    filtered_df = df[df[column] >= min_score]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "def filter_by_mhc_class(df, column='mhc.class'):\n",
    "    \"\"\"\n",
    "    Filters the DataFrame based on a user-specified MHC class ('MHCI' or 'MHCII').\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The data frame to filter.\n",
    "    column (str): The name of the column to apply the filter on. Defaults to 'mhc.class'.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A DataFrame filtered based on the user-specified MHC class.\n",
    "    \"\"\"\n",
    "    # Prompting user for MHC class\n",
    "    mhc_class = input(f\"Enter the MHC class to keep ('MHCI' or 'MHCII'): \").strip()\n",
    "\n",
    "    # Validating user input\n",
    "    if mhc_class not in ['MHCI', 'MHCII']:\n",
    "        print(\"Invalid input. Please enter 'MHCI' or 'MHCII'.\")\n",
    "        return df\n",
    "    \n",
    "    # Filtering the DataFrame\n",
    "    filtered_df = df[df[column] == mhc_class]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df = df_cleaned\n",
    "# Apply length range filters\n",
    "df_filtered_alpha = filter_by_length_range(df, 'cdr3.alpha')\n",
    "df_filtered_beta = filter_by_length_range(df, 'cdr3.beta')\n",
    "df_filtered_epitope = filter_by_length_range(df, 'antigen.epitope')\n",
    "df_filtered_min_score = filter_by_minimum_score(df)\n",
    "# Intersect the filtered DataFrames to get only rows that meet all criteria\n",
    "df_length_filtered = df_filtered_alpha.merge(df_filtered_beta).merge(df_filtered_epitope)\n",
    "\n",
    "# Further filter by species\n",
    "df_final_filtered = filter_by_species(df_length_filtered)\n",
    "\n",
    "# df_final_filtered is now your preprocessed DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a31ec6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cdr3.alpha                0\n",
      "v.alpha                   0\n",
      "cdr3.beta                 0\n",
      "v.beta                    0\n",
      "species                   0\n",
      "mhc.a                     0\n",
      "mhc.b                     0\n",
      "antigen.gene              8\n",
      "antigen.epitope           0\n",
      "vdjdb.score               0\n",
      "mhc.class                 0\n",
      "cdr3.alpha.length         0\n",
      "cdr3.beta.length          0\n",
      "antigen.epitope.length    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dg/1nsyrpm96v149hfhy4g6qc0h0000gn/T/ipykernel_58657/1803294583.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final_filtered.drop(['j.beta'], axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df_final_filtered\n",
    "\n",
    "# df_final_filtered.drop(['d.beta'], axis=1, inplace=True)\n",
    "\n",
    "df_final_filtered.drop(['j.beta'], axis=1, inplace=True)\n",
    "\n",
    "print(df_final_filtered.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f19ba6",
   "metadata": {},
   "source": [
    "# CDR3 seq TCRdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4864e3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING: curl -o /Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/tcrsampler/db/wiraninha_sampler.zip https://www.dropbox.com/s/ily0td3tn1uc7bi/wiraninha_sampler.zip?dl=1 -L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    66    0    66    0     0    207      0 --:--:-- --:--:-- --:--:--   207\n",
      "100   320  100   320    0     0    407      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 6882k  100 6882k    0     0  2412k      0  0:00:02  0:00:02 --:--:-- 6554k\n",
      "replace /Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/tcrsampler/db/wirasinha_mouse_alpha_g8a.tsv.sampler.tsv? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n",
      "(EOF or read error, treating as \"[N]one\" ...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/tcrsampler/db/wiraninha_sampler.zip\n",
      "RUNNING: curl -o /Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/tcrsampler/db/ravens_samplers.zip https://www.dropbox.com/s/bahxa6x86drq0n5/ravens_samplers.zip?dl=1 -L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    64    0    64    0     0    129      0 --:--:-- --:--:-- --:--:--   130\n",
      "100   320  100   320    0     0    250      0  0:00:01  0:00:01 --:--:--     0\n",
      "100  313k  100  313k    0     0   181k      0  0:00:01  0:00:01 --:--:--  181k\n",
      "replace /Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/tcrsampler/db/ravens_human_gamma_t.sampler.tsv? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n",
      "(EOF or read error, treating as \"[N]one\" ...)\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/tcrsampler/db/ravens_samplers.zip\n",
      "RUNNING: curl -o /Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/tcrsampler/db/olga_sampler.zip https://www.dropbox.com/s/qlsxvst8bn04l0n/olga_sampler.zip?dl=1 -L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100    61    0    61    0     0    109      0 --:--:-- --:--:-- --:--:--   108\n",
      "100   320  100   320    0     0    285      0  0:00:01  0:00:01 --:--:--     0\n",
      "100 23.5M  100 23.5M    0     0  5612k      0  0:00:04  0:00:04 --:--:-- 9529k\n",
      "replace /Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/tcrsampler/db/olga_human_beta_t.sampler.tsv? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n",
      "(EOF or read error, treating as \"[N]one\" ...)\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/tcrsampler/db/olga_sampler.zip\n",
      "RUNNING: curl -o /Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/tcrsampler/db/ruggiero_mouse_sampler.zip https://www.dropbox.com/s/yz8v1c1gf2eyzxk/ruggiero_mouse_sampler.zip?dl=1 -L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100    71    0    71    0     0    220      0 --:--:-- --:--:-- --:--:--   220\n",
      "100   320  100   320    0     0    386      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  210k  100  210k    0     0   158k      0  0:00:01  0:00:01 --:--:--  158k\n",
      "replace /Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/tcrsampler/db/ruggiero_mouse_alpha_t.tsv.sampler.tsv? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n",
      "(EOF or read error, treating as \"[N]one\" ...)\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/tcrsampler/db/ruggiero_mouse_sampler.zip\n",
      "RUNNING: curl -o /Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/tcrsampler/db/ruggiero_human_sampler.zip https://www.dropbox.com/s/jda6qtemk65zlfk/ruggiero_human_sampler.zip?dl=1 -L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100    71    0    71    0     0    226      0 --:--:-- --:--:-- --:--:--   226\n",
      "100   320  100   320    0     0    109      0  0:00:02  0:00:02 --:--:--   199\n",
      "100  599k  100  599k    0     0   173k      0  0:00:03  0:00:03 --:--:-- 6867k\n",
      "replace /Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/tcrsampler/db/ruggiero_human_alpha_t.tsv.sampler.tsv? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n",
      "(EOF or read error, treating as \"[N]one\" ...)\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/tcrsampler/db/ruggiero_human_sampler.zip\n",
      "RUNNING: curl -o /Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/tcrsampler/db/emerson_human_beta_t_cmvneg.tsv.sampler.tsv.zip https://www.dropbox.com/s/04mxrzw7f5wkg1x/emerson_human_beta_t_cmvneg.tsv.sampler.tsv.zip?dl=1 -L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100    92    0    92    0     0    193      0 --:--:-- --:--:-- --:--:--   193\n",
      "100   320  100   320    0     0    293      0  0:00:01  0:00:01 --:--:--     0\n",
      "100 11.3M  100 11.3M    0     0  4318k      0  0:00:02  0:00:02 --:--:-- 12.7M\n",
      "replace /Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/tcrsampler/db/emerson_human_beta_t_cmvneg.tsv.sampler.tsv? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n",
      "(EOF or read error, treating as \"[N]one\" ...)\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/tcrsampler/db/emerson_human_beta_t_cmvneg.tsv.sampler.tsv.zip\n",
      "RUNNING: curl -o /Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/tcrsampler/db/britanova_human_beta_t_cb.tsv.sampler.tsv.zip https://www.dropbox.com/s/87n5v2by80xhy1q/britanova_human_beta_t_cb.tsv.sampler.tsv.zip?dl=1 -L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100    90    0    90    0     0    325      0 --:--:-- --:--:-- --:--:--   326\n",
      "100   320  100   320    0     0    366      0 --:--:-- --:--:-- --:--:--   366\n",
      "100 28.3M  100 28.3M    0     0  6234k      0  0:00:04  0:00:04 --:--:-- 8181k\n",
      "replace /Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/tcrsampler/db/britanova_human_beta_t_cb.tsv.sampler.tsv? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n",
      "(EOF or read error, treating as \"[N]one\" ...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/tcrsampler/db/britanova_human_beta_t_cb.tsv.sampler.tsv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/base/tsa_model.py:7: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n",
      "/Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/base/tsa_model.py:7: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n"
     ]
    }
   ],
   "source": [
    "from tcrsampler.setup_db import install_all_next_gen\n",
    "install_all_next_gen(dry_run = False)\n",
    "from tcrdist.rep_funcs import _pws, _pw  \n",
    "from tcrdist.repertoire import TCRrep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20b9ea94",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dg/1nsyrpm96v149hfhy4g6qc0h0000gn/T/ipykernel_54338/2049687273.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final_filtered.rename(columns={'cdr3.alpha': 'cdr3_a_aa', 'cdr3.beta': 'cdr3_b_aa', 'v.alpha':'v_a_gene','j.alpha': 'j_a_gene','v.beta': 'v_b_gene','j.beta': 'j_b_gene','antigen.epitope':'epitope'}, inplace=True)\n",
      "/Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/tcrdist/repertoire.py:159: UserWarning: cell_df needs a counts column to track clonal number of frequency\n",
      "\n",
      "  self._validate_cell_df()\n",
      "/Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/tcrdist/repertoire.py:791: UserWarning: No 'count' column provided; count column set to 1\n",
      "  warnings.warn(\"No 'count' column provided; count column set to 1\")\n",
      "/Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/tcrdist/repertoire.py:792: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.cell_df['count'] = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8816, 13)\n",
      "[[ 0 24 24 ... 24 31 32]\n",
      " [24  0  0 ... 20 27 23]\n",
      " [24  0  0 ... 20 27 23]\n",
      " ...\n",
      " [24 20 20 ...  0 23 20]\n",
      " [31 27 27 ... 23  0 24]\n",
      " [32 23 23 ... 20 24  0]]\n",
      "(8816, 8816)\n"
     ]
    }
   ],
   "source": [
    "df_final_filtered.rename(columns={'cdr3.alpha': 'cdr3_a_aa', 'cdr3.beta': 'cdr3_b_aa', 'v.alpha':'v_a_gene','j.alpha': 'j_a_gene','v.beta': 'v_b_gene','j.beta': 'j_b_gene','antigen.epitope':'epitope'}, inplace=True)\n",
    "\n",
    "print(np.shape(df_final_filtered))\n",
    "def cdr3plot2(df_):\n",
    "    # Assuming TCRrep is correctly set up to work with the provided dataframe\n",
    "    tr_vdjdb = TCRrep(cell_df=df_, \n",
    "                      organism='human',\n",
    "                      chains=['beta', 'alpha'],\n",
    "                      deduplicate=False,\n",
    "                      compute_distances=True )\n",
    "    \n",
    "    tcrdist_matrix = tr_vdjdb.pw_cdr3_b_aa\n",
    "    return tcrdist_matrix\n",
    "\n",
    "tcrdist_matrix =  cdr3plot2(df_final_filtered)\n",
    "print(tcrdist_matrix)\n",
    "print(np.shape(tcrdist_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4db903",
   "metadata": {},
   "source": [
    "Below code accounts for large datasets ( >10000 ), but the tcrdist only provides sparse matrix for this number of datapoints. Dont know how to label with a sparse dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76d6a6bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/tcrdist/repertoire.py:201: UserWarning: \n",
      "\n",
      "When TCRrep.<clone_df> size 11078 > 10,000.\n",
      "\tTCRrep.compute_distances() may be called explicitly by a user\n",
      "\twith knowledge of system memory availability.\n",
      "\tHowever, it's HIGHLY unlikely that you want to compute such\n",
      "\ta large numpy array. INSTEAD, if you want all pairwise distance,\n",
      "\tyou will likely want to set an appropriate number of cpus with TCRrep.cpus = x,\n",
      "\tand then generate a scipy.sparse csr matrix of distances with:\n",
      "\tTCRrep.compute_sparse_rect_distances(radius=50, chunk_size=100), leaving df and df2 arguments blank.\n",
      "\tWhen you do this the results will be stored as TCRrep.rw_beta instead of TCRrep.pw_beta.\n",
      "\tThis function is highly useful for comparing a smaller number of sequences against a bulk set\n",
      "\tIn such a case, you can specify df and df2 arguments to create a non-square matrix of distances.\n",
      "\tSee https://tcrdist3.readthedocs.io/en/latest/sparsity.html?highlight=sparse for more info.\n",
      "\n",
      "  warnings.warn(f\"\\n\\nWhen TCRrep.<clone_df> size {self.clone_df.shape[0]} > 10,000.\\n\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd691efede0e47ffb2ea8fc9a0391a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/111 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/base/tsa_model.py:7: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n",
      "/Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/base/tsa_model.py:7: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n",
      "/Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/base/tsa_model.py:7: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n",
      "/Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/base/tsa_model.py:7: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n",
      "/Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/base/tsa_model.py:7: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n",
      "/Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/base/tsa_model.py:7: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n",
      "/Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/base/tsa_model.py:7: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n",
      "/Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/base/tsa_model.py:7: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0c9e3bd5ac4516b46beaab57a5a389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/111 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/base/tsa_model.py:7: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n",
      "/Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/base/tsa_model.py:7: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n",
      "/Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/base/tsa_model.py:7: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n",
      "/Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/base/tsa_model.py:7: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n",
      "/Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/base/tsa_model.py:7: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n",
      "/Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/base/tsa_model.py:7: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n",
      "/Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/base/tsa_model.py:7: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n",
      "/Users/tomelder/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/base/tsa_model.py:7: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t-1\n",
      "  (1, 1)\t-1\n",
      "  (1, 1332)\t42\n",
      "  (1, 10800)\t48\n",
      "  (1, 10841)\t48\n",
      "  (2, 2)\t-1\n",
      "  (2, 4)\t30\n",
      "  (3, 3)\t-1\n",
      "  (3, 244)\t48\n",
      "  (3, 10956)\t48\n",
      "  (3, 10976)\t36\n",
      "  (3, 10987)\t48\n",
      "  (4, 2)\t30\n",
      "  (4, 4)\t-1\n",
      "  (4, 1397)\t48\n",
      "  (4, 9652)\t48\n",
      "  (4, 11034)\t48\n",
      "  (5, 5)\t-1\n",
      "  (5, 6)\t36\n",
      "  (6, 5)\t36\n",
      "  (6, 6)\t-1\n",
      "  (6, 7)\t36\n",
      "  (6, 710)\t48\n",
      "  (7, 6)\t36\n",
      "  (7, 7)\t-1\n",
      "  :\t:\n",
      "  (11067, 1057)\t48\n",
      "  (11067, 1058)\t48\n",
      "  (11067, 9678)\t12\n",
      "  (11067, 11067)\t-1\n",
      "  (11068, 11068)\t-1\n",
      "  (11069, 11069)\t-1\n",
      "  (11070, 284)\t36\n",
      "  (11070, 285)\t36\n",
      "  (11070, 10911)\t48\n",
      "  (11070, 11070)\t-1\n",
      "  (11071, 11071)\t-1\n",
      "  (11072, 11072)\t-1\n",
      "  (11073, 9452)\t48\n",
      "  (11073, 9710)\t36\n",
      "  (11073, 10105)\t48\n",
      "  (11073, 11073)\t-1\n",
      "  (11074, 11074)\t-1\n",
      "  (11075, 281)\t30\n",
      "  (11075, 11031)\t48\n",
      "  (11075, 11075)\t-1\n",
      "  (11076, 10760)\t42\n",
      "  (11076, 10761)\t42\n",
      "  (11076, 10820)\t21\n",
      "  (11076, 11076)\t-1\n",
      "  (11077, 11077)\t-1\n",
      "(11078, 11078)\n"
     ]
    }
   ],
   "source": [
    "def cdr3plot2(df_):\n",
    "    # Assuming TCRrep is correctly set up to work with the provided dataframe\n",
    "    tr_vdjdb = TCRrep(cell_df=df_, \n",
    "                      organism='human',\n",
    "                      chains=['beta', 'alpha'],\n",
    "                      deduplicate=False)\n",
    "\n",
    "    # Check if the size exceeds 10,000, to manage memory usage more efficiently\n",
    "    if len(df_) > 10000:\n",
    "        # Set the number of CPUs to use for parallel computation\n",
    "        tr_vdjdb.cpus = 4  # Adjust this number based on your system's capabilities\n",
    "\n",
    "        # Compute sparse rectangular distances for large datasets\n",
    "        # The radius parameter can be adjusted based on your specific requirements\n",
    "        tr_vdjdb.compute_sparse_rect_distances(radius=50, chunk_size=100)\n",
    "\n",
    "        # Accessing the sparse matrix of distances stored in rw_beta\n",
    "        tcrdist_matrix = tr_vdjdb.rw_beta\n",
    "        \n",
    "        from scipy.sparse import csr_matrix\n",
    "\n",
    "        # Assuming `tcrdist_matrix` is your sparse matrix\n",
    "        tcrdist_matrix_csr = csr_matrix(tcrdist_matrix)\n",
    "    else:\n",
    "        # For smaller datasets, proceed with the standard distance computation\n",
    "        tr_vdjdb.compute_distances()\n",
    "        tcrdist_matrix = tr_vdjdb.pw_cdr3_b_aa\n",
    "\n",
    "    return tcrdist_matrix\n",
    "tcrdist_matrix =  cdr3plot2(df_final_filtered)\n",
    "print(tcrdist_matrix)\n",
    "print(np.shape(tcrdist_matrix))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6922df32",
   "metadata": {},
   "source": [
    "### DBscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5a609ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of DBscan clusters: 31\n",
      "Number of Epitopes: 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dg/1nsyrpm96v149hfhy4g6qc0h0000gn/T/ipykernel_54338/947410217.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final_filtered['cluster'] = clusters\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(metric='precomputed', eps=0.2, min_samples=2)  # Example: eps value here is hypothetical\n",
    "clusters = dbscan.fit_predict(tcrdist_matrix)\n",
    "\n",
    "# Adding cluster labels to your data (assuming you have a DataFrame 'df' with your TCR sequence data)\n",
    "df_final_filtered['cluster'] = clusters\n",
    "\n",
    "df_final_filtered\n",
    "\n",
    "# Exclude outlier points and count unique clusters\n",
    "n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)\n",
    "\n",
    "print(f\"Number of DBscan clusters: {n_clusters}\")\n",
    "\n",
    "print(f\"Number of Epitopes: {len(df_final_filtered['epitope'].unique())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82deacf",
   "metadata": {},
   "source": [
    "### Random classifier for baseline\n",
    "\n",
    "Random Clustering:\n",
    "\n",
    "The model randomly assigns TCR sequences to a predetermined number of clusters, mirroring the number your actual clustering algorithm produces. This process simulates a scenario where clusters are formed without any underlying biological or data-driven rationale.\n",
    "Calculating Purity and Consistency:\n",
    "\n",
    "Purity: For each randomly formed cluster, the model calculates the proportion of the most common epitope in that cluster. The idea is to see, on average, how often random clustering accidentally groups sequences by epitope. High purity in random clustering indicates that the dataset's imbalance might significantly influence purity scores.\n",
    "Consistency: This measures how often sequences that bind to the same epitope end up in the same cluster, based on the random assignments. It assesses if the random clusters align sequences by epitope as effectively as the actual method.\n",
    "Simulation Over Multiple Iterations:\n",
    "\n",
    "The process of random clustering and subsequent evaluation of purity and consistency is repeated multiple times (e.g., 50). This repetition helps smooth out variances due to the randomness, providing a more reliable average baseline for these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83713993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_clustering(df, n_clusters):\n",
    "    \"\"\"Assigns each row in the dataframe to a random cluster.\"\"\"\n",
    "    random_clusters = np.random.randint(0, n_clusters, len(df))\n",
    "    df['random_cluster'] = random_clusters\n",
    "    return df\n",
    "\n",
    "def calculate_random_purity(df):\n",
    "    purity_sum = 0\n",
    "    for cluster in df['random_cluster'].unique():\n",
    "        cluster_df = df[df['random_cluster'] == cluster]\n",
    "        most_common_epitope = cluster_df['epitope'].value_counts().idxmax()\n",
    "        purity_sum += cluster_df['epitope'].value_counts().max() / len(cluster_df)\n",
    "    purity = purity_sum / len(df['random_cluster'].unique())\n",
    "    return purity\n",
    "\n",
    "def calculate_random_consistency(df, epitope_clusters):\n",
    "    correct_assignments = 0\n",
    "    for epitope, cluster in epitope_clusters.items():\n",
    "        correct_assignments += len(df[(df['epitope'] == epitope) & (df['random_cluster'] == cluster)])\n",
    "    consistency = correct_assignments / len(df)\n",
    "    return consistency\n",
    "\n",
    "def simulate_baseline(df, n_clusters, iterations=50):\n",
    "    purity_scores = []\n",
    "    consistency_scores = []\n",
    "    def get_epitope_clusters(df):\n",
    "        epitope_clusters = {}\n",
    "        for epitope in df['epitope'].unique():\n",
    "            epitope_df = df[df['epitope'] == epitope]\n",
    "            most_common_cluster = epitope_df['cluster'].value_counts().idxmax()\n",
    "            epitope_clusters[epitope] = most_common_cluster\n",
    "        return epitope_clusters\n",
    "\n",
    "    # Assuming df_final_filtered has 'cluster' column from DBSCAN and 'epitope' information\n",
    "    epitope_clusters = get_epitope_clusters(df_final_filtered)\n",
    "\n",
    "  # This should come from your actual clustering method\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        df_random = random_clustering(df.copy(), n_clusters)\n",
    "        purity = calculate_random_purity(df_random)\n",
    "        consistency = calculate_random_consistency(df_random, epitope_clusters)\n",
    "        \n",
    "        purity_scores.append(purity)\n",
    "        consistency_scores.append(consistency)\n",
    "    \n",
    "    return np.mean(purity_scores), np.mean(consistency_scores)\n",
    "\n",
    "pur_base, consist_base = simulate_baseline(df_final_filtered, n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b078731a",
   "metadata": {},
   "source": [
    "### Evaluate DBscan clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a32688f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retention: 0.9878629764065335\n",
      "Purity: 0.9114656735829082\n",
      "Consistency: 0.9799058445286485\n",
      "Purity baseline: 0.969157044703137\n",
      "Consistency baseline: 0.031801270417422874\n"
     ]
    }
   ],
   "source": [
    "def calculate_retention(df):\n",
    "    # Retention is the fraction of TCR sequences assigned to any cluster\n",
    "    assigned = df[df['cluster'] != -1]  # DBSCAN labels outliers as -1\n",
    "    retention = len(assigned) / len(df)\n",
    "    return retention\n",
    "\n",
    "def calculate_purity(df):\n",
    "    # Purity is defined for each cluster, then averaged across clusters\n",
    "    purity_sum = 0\n",
    "    for cluster in df['cluster'].unique():\n",
    "        if cluster == -1:\n",
    "            continue  # Skip noise points\n",
    "        cluster_df = df[df['cluster'] == cluster]\n",
    "        most_common_epitope = cluster_df['epitope'].value_counts().idxmax()\n",
    "        purity_sum += cluster_df['epitope'].value_counts().max() / len(cluster_df)\n",
    "    purity = purity_sum / (len(df['cluster'].unique()) - (1 if -1 in df['cluster'].unique() else 0))\n",
    "    return purity\n",
    "\n",
    "def calculate_consistency(df):\n",
    "    # Consistency is calculated based on the assignment of TCR sequences to the \"true\" cluster for their epitope\n",
    "    epitope_clusters = {}\n",
    "    for epitope in df['epitope'].unique():\n",
    "        epitope_df = df[df['epitope'] == epitope]\n",
    "        most_common_cluster = epitope_df['cluster'].value_counts().idxmax()\n",
    "        epitope_clusters[epitope] = most_common_cluster\n",
    "\n",
    "    correct_assignments = 0\n",
    "    for epitope, cluster in epitope_clusters.items():\n",
    "        correct_assignments += len(df[(df['epitope'] == epitope) & (df['cluster'] == cluster)])\n",
    "\n",
    "    consistency = correct_assignments / len(df[df['cluster'] != -1])\n",
    "    return consistency\n",
    "\n",
    "\n",
    "\n",
    "retention = calculate_retention(df_final_filtered)\n",
    "purity = calculate_purity(df_final_filtered)\n",
    "consistency = calculate_consistency(df_final_filtered)\n",
    "\n",
    "print(f\"Retention: {retention}\")\n",
    "print(f\"Purity: {purity}\")\n",
    "print(f\"Consistency: {consistency}\")\n",
    "\n",
    "print(f\"Purity baseline: {pur_base}\")\n",
    "print(f\"Consistency baseline: {consist_base}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134e2cf9",
   "metadata": {},
   "source": [
    "## Supervised KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60341473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dg/1nsyrpm96v149hfhy4g6qc0h0000gn/T/ipykernel_1741/3946490072.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final_filtered.rename(columns={'cdr3.alpha': 'cdr3_a_aa', 'cdr3.beta': 'cdr3_b_aa', 'v.alpha':'v_a_gene','j.alpha': 'j_a_gene','v.beta': 'v_b_gene','j.beta': 'j_b_gene','antigen.epitope':'epitope'}, inplace=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Precomputed matrix must be a square matrix. Input is a 7866x9833 matrix.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dg/1nsyrpm96v149hfhy4g6qc0h0000gn/T/ipykernel_1741/3946490072.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Train the KNN model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mknn_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'precomputed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mknn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mfitted\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnearest\u001b[0m \u001b[0mneighbors\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;31m# Precomputed matrix X must be squared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'precomputed'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m             raise ValueError(\"Precomputed matrix must be a square matrix.\"\n\u001b[0m\u001b[1;32m    457\u001b[0m                              \u001b[0;34m\" Input is a {}x{} matrix.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                              .format(X.shape[0], X.shape[1]))\n",
      "\u001b[0;31mValueError\u001b[0m: Precomputed matrix must be a square matrix. Input is a 7866x9833 matrix."
     ]
    }
   ],
   "source": [
    "from tcrdist.repertoire import TCRrep\n",
    "df_final_filtered.rename(columns={'cdr3.alpha': 'cdr3_a_aa', 'cdr3.beta': 'cdr3_b_aa', 'v.alpha':'v_a_gene','j.alpha': 'j_a_gene','v.beta': 'v_b_gene','j.beta': 'j_b_gene','antigen.epitope':'epitope'}, inplace=True)\n",
    "\n",
    "# Initialize TCRrep object\n",
    "tcr_rep = TCRrep(\n",
    "    cell_df = df_final_filtered,\n",
    "    organism = 'human', # Adjust based on your data, or dynamically set based on the 'species' column\n",
    "    chains = ['alpha', 'beta'],\n",
    "    compute_distances = True,\n",
    "    deduplicate = False \n",
    ")\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Assume X (distance matrix) is derived from tcr_rep object\n",
    "# X = pd.DataFrame(tcr_rep.pw_alpha + tcr_rep.pw_beta)\n",
    "X = pd.DataFrame(tcr_rep.pw_beta)\n",
    "Y = df_final_filtered['epitope'] # Or any other column you wish to predict\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the KNN model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5, metric='precomputed')\n",
    "knn_model.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = knn_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "\n",
    "# Assuming predictions and Y_test are already defined from the model testing\n",
    "# Note: Adapt the metrics calculation as needed based on your specific task (binary vs. multiclass classification)\n",
    "\n",
    "# Calculate and print common evaluation metrics\n",
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "precision = precision_score(Y_test, predictions, average='macro')  # Use 'binary' for binary classification\n",
    "recall = recall_score(Y_test, predictions, average='macro')  # Use 'binary' for binary classification\n",
    "f1 = f1_score(Y_test, predictions, average='macro')  # Use 'binary' for binary classification\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# For binary classification, you may also want to compute the AUC-ROC\n",
    "# Ensure your problem is binary and y_true and y_scores are appropriately defined for this metric\n",
    "# For multi-class scenarios, consider a one-vs-rest approach to compute AUC-ROC\n",
    "if len(Y_test.unique()) == 2:  # Checking if it's a binary classification\n",
    "    # Assuming `prob_predictions` contain probability scores of the positive class\n",
    "    prob_predictions = knn_model.predict_proba(X_test)[:, 1]  # Adapt this indexing based on your classifier output\n",
    "    auc_roc = roc_auc_score(Y_test, prob_predictions)\n",
    "    print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
    "\n",
    "# For a detailed classification report (precision, recall, f1-score per class)\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(Y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b171c971",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
