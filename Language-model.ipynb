{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Language models for TCR specificity prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61636, 34)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christian Gauthier\\AppData\\Local\\Temp\\ipykernel_29732\\1797484951.py:4: DtypeWarning: Columns (30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, delimiter='\\t')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"vdjdb_full.txt\"\n",
    "df = pd.read_csv(file_path, delimiter='\\t')\n",
    "df = df.drop_duplicates()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       cdr3.alpha             cdr3.beta      species antigen.epitope  \\\n",
      "0   CIVRAPGRADMRF  CASSYLPGQGDHYSNQPQHF  HomoSapiens        FLKEKGGL   \n",
      "1             NaN   CASSFEAGQGFFSNQPQHF  HomoSapiens        FLKEKGGL   \n",
      "2  CAVPSGAGSYQLTF   CASSFEPGQGFYSNQPQHF  HomoSapiens        FLKEKGGL   \n",
      "3     CAVKASGSRLT  CASSYEPGQVSHYSNQPQHF  HomoSapiens        FLKEKGGL   \n",
      "4   CAYRPPGTYKYIF        CASSALASLNEQFF  HomoSapiens        FLKEKGGL   \n",
      "\n",
      "  antigen.gene  vdjdb.score  \n",
      "0          Nef            2  \n",
      "1          Nef            2  \n",
      "2          Nef            2  \n",
      "3          Nef            2  \n",
      "4          Nef            2  \n",
      "(9300, 6)\n"
     ]
    }
   ],
   "source": [
    "df = df[(df['vdjdb.score'] > 0)]\n",
    "df_slim = df[['cdr3.alpha','cdr3.beta','species','antigen.epitope','antigen.gene','vdjdb.score']]\n",
    "print(df_slim.head(5))\n",
    "print(df_slim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            cdr3.alpha          cdr3.beta      species antigen.epitope  \\\n",
      "19746    CAGAIPRDDKIIF   CASSLNPGRSDSPLHF  HomoSapiens       AAFKRSCLK   \n",
      "19745  CALATHTGTASKLTF   CASSQDPGSSYNEQFF  HomoSapiens       AAFKRSCLK   \n",
      "19744     CAGARNDYKLSF  CATSRDGAGLVNQPQHF  HomoSapiens       AAFKRSCLK   \n",
      "\n",
      "      antigen.gene  vdjdb.score                     cdr3combined  \n",
      "19746         T-Ag            3    CAGAIPRDDKIIFCASSLNPGRSDSPLHF  \n",
      "19745         T-Ag            3  CALATHTGTASKLTFCASSQDPGSSYNEQFF  \n",
      "19744         T-Ag            3    CAGARNDYKLSFCATSRDGAGLVNQPQHF  \n",
      "CIVRAPGRADMRFCASSYLPGQGDHYSNQPQHF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christian Gauthier\\AppData\\Local\\Temp\\ipykernel_29732\\3123875043.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_slim['cdr3combined'] = df_slim['cdr3.alpha'].fillna('') + df_slim['cdr3.beta'].fillna('')\n"
     ]
    }
   ],
   "source": [
    "#Stick CRD3 together if both alpha and beta present if not leave alone\n",
    "df_slim['cdr3combined'] = df_slim['cdr3.alpha'].fillna('') + df_slim['cdr3.beta'].fillna('')\n",
    "cdr3comb = df_slim['cdr3combined'].dropna()\n",
    "cdr3comb = cdr3comb.reset_index(drop=True)\n",
    "df_slim = df_slim.sort_values('antigen.epitope')\n",
    "print(df_slim.head(3))\n",
    "print(cdr3comb[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "\n",
    "class TCRBERT(tf.keras.Model):\n",
    "    def __init__(self, hidden_dim=768, intermediate_dim=3072, num_attention_heads=12, num_transformer_layers=12):\n",
    "        super(TCRBERT, self).__init__()\n",
    "        self.bert = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "        self.transformer_blocks = [TransformerBlock(hidden_dim, intermediate_dim, num_attention_heads) \n",
    "                                   for _ in range(num_transformer_layers)]\n",
    "        self.maa_head = tf.keras.layers.Dense(20, activation='softmax')  # 20 amino acids\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        sequence_output, _ = self.bert(inputs)  # Get BERT sequence output\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            sequence_output = transformer_block(sequence_output, training=training)  # Apply transformer blocks\n",
    "        logits = self.maa_head(sequence_output)  # MAA head\n",
    "        return logits\n",
    "\n",
    "#Transformer Block\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_dim, intermediate_dim, num_attention_heads):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(num_heads=num_attention_heads, key_dim=hidden_dim)\n",
    "        self.feed_forward = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(intermediate_dim, activation='relu'),\n",
    "            tf.keras.layers.Dense(hidden_dim)\n",
    "        ])\n",
    "        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(0.1)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(0.1)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attention_output = self.attention(inputs, inputs)\n",
    "        attention_output = self.dropout1(attention_output, training=training)\n",
    "        attention_output = self.layer_norm1(inputs + attention_output)\n",
    "        feed_forward_output = self.feed_forward(attention_output)\n",
    "        feed_forward_output = self.dropout2(feed_forward_output, training=training)\n",
    "        sequence_output = self.layer_norm2(attention_output + feed_forward_output)\n",
    "        return sequence_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_slim['cdr3combined']\n",
    "y = df_slim['antigen.epitope']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "#tokenize the sequences\n",
    "inputs = tokenizer.batch_encode_plus(\n",
    "    cdr3comb,\n",
    "    add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "    return_attention_mask=True,\n",
    "    padding='longest',  # Pad all sentences to the length of the longest sentence\n",
    "    truncation=True,\n",
    "    max_length=512,  # Max length to pad sequences to\n",
    ")\n",
    "\n",
    "# Get the input IDs (tokenized sequences), attention masks and labels\n",
    "input_ids = inputs['input_ids']\n",
    "attention_masks = inputs['attention_mask']\n",
    "labels = df_slim['antigen.epitope']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first value of 'input_ids' is: [  101  6187 23805 18098 14141  3211 10128 15671 14540 16275 16523 16150\n",
      " 13102  2140  2232  2546   102     0     0     0     0     0     0     0]\n",
      "The first value of 'token_type_ids' is: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "The first value of 'attention_mask' is: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "for key, tensor in inputs.items():\n",
    "    first_value = tensor.numpy()[0]\n",
    "    print(f\"The first value of '{key}' is: {first_value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
