{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Language models for TCR specificity prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christian Gauthier\\AppData\\Local\\Temp\\ipykernel_26828\\568133031.py:8: DtypeWarning: Columns (30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, delimiter='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61636, 34)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "\n",
    "file_path = \"vdjdb_full.txt\"\n",
    "df = pd.read_csv(file_path, delimiter='\\t')\n",
    "df = df.drop_duplicates()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       cdr3.alpha             cdr3.beta      species antigen.epitope  \\\n",
      "0   CIVRAPGRADMRF  CASSYLPGQGDHYSNQPQHF  HomoSapiens        FLKEKGGL   \n",
      "1             NaN   CASSFEAGQGFFSNQPQHF  HomoSapiens        FLKEKGGL   \n",
      "2  CAVPSGAGSYQLTF   CASSFEPGQGFYSNQPQHF  HomoSapiens        FLKEKGGL   \n",
      "3     CAVKASGSRLT  CASSYEPGQVSHYSNQPQHF  HomoSapiens        FLKEKGGL   \n",
      "4   CAYRPPGTYKYIF        CASSALASLNEQFF  HomoSapiens        FLKEKGGL   \n",
      "\n",
      "  antigen.gene  vdjdb.score  \n",
      "0          Nef            2  \n",
      "1          Nef            2  \n",
      "2          Nef            2  \n",
      "3          Nef            2  \n",
      "4          Nef            2  \n",
      "(9300, 6)\n"
     ]
    }
   ],
   "source": [
    "df = df[(df['vdjdb.score'] > 0)]\n",
    "df_slim = df[['cdr3.alpha','cdr3.beta','species','antigen.epitope','antigen.gene','vdjdb.score']]\n",
    "print(df_slim.head(5))\n",
    "print(df_slim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            cdr3.alpha          cdr3.beta      species antigen.epitope  \\\n",
      "19746    CAGAIPRDDKIIF   CASSLNPGRSDSPLHF  HomoSapiens       AAFKRSCLK   \n",
      "19745  CALATHTGTASKLTF   CASSQDPGSSYNEQFF  HomoSapiens       AAFKRSCLK   \n",
      "19744     CAGARNDYKLSF  CATSRDGAGLVNQPQHF  HomoSapiens       AAFKRSCLK   \n",
      "\n",
      "      antigen.gene  vdjdb.score                     cdr3combined  \n",
      "19746         T-Ag            3    CAGAIPRDDKIIFCASSLNPGRSDSPLHF  \n",
      "19745         T-Ag            3  CALATHTGTASKLTFCASSQDPGSSYNEQFF  \n",
      "19744         T-Ag            3    CAGARNDYKLSFCATSRDGAGLVNQPQHF  \n",
      "CIVRAPGRADMRFCASSYLPGQGDHYSNQPQHF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christian Gauthier\\AppData\\Local\\Temp\\ipykernel_26828\\3123875043.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_slim['cdr3combined'] = df_slim['cdr3.alpha'].fillna('') + df_slim['cdr3.beta'].fillna('')\n"
     ]
    }
   ],
   "source": [
    "#Stick CRD3 together if both alpha and beta present if not leave alone\n",
    "df_slim['cdr3combined'] = df_slim['cdr3.alpha'].fillna('') + df_slim['cdr3.beta'].fillna('')\n",
    "cdr3comb = df_slim['cdr3combined'].dropna()\n",
    "cdr3comb = cdr3comb.reset_index(drop=True)\n",
    "df_slim = df_slim.sort_values('antigen.epitope')\n",
    "print(df_slim.head(3))\n",
    "print(cdr3comb[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "max_length = max(cdr3comb, key=lambda x: len(x))\n",
    "print(len(max_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Christian Gauthier\\miniconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Christian Gauthier\\AppData\\Local\\Temp\\ipykernel_26828\\2164266788.py:17: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  tokenized_inputs = tokenizer(df_slim['cdr3combined'][0:1500].tolist(), padding='max_length', truncation=True, max_length=38) #1500 is max before ResourceExhaustedError on local machine\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "class TCRBertModel(tf.keras.Model):\n",
    "    def __init__(self, bert_model_name='bert-base-uncased'): #uncased produces 728 dimensional embeddings\n",
    "        super(TCRBertModel, self).__init__()\n",
    "        self.bert = TFBertModel.from_pretrained(bert_model_name)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        outputs = self.bert(inputs)\n",
    "        sequence_output = outputs[0]\n",
    "        return sequence_output\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') #load the tokenizer for the BERT model\n",
    "\n",
    "#tokenize the TCR sequences\n",
    "tokenized_inputs = tokenizer(df_slim['cdr3combined'][0:1500].tolist(), padding='max_length', truncation=True, max_length=38) #1500 is max before ResourceExhaustedError on local machine\n",
    "\n",
    "#convert the tokenized sequences to tensors\n",
    "input_ids = tf.constant(tokenized_inputs['input_ids'])\n",
    "attention_mask = tf.constant(tokenized_inputs['attention_mask']) #decides what should not be attended to as some are tokens are padded to 38 length\n",
    "\n",
    "#dictionary mapping input names to their values\n",
    "inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "\n",
    "model = TCRBertModel()\n",
    "\n",
    "#get 768-dimensional embeddings for each token in the input sequences\n",
    "outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-6.74676895e-01 -9.55889672e-02  2.27257103e-01 ... -2.54138976e-01\n",
      "    3.10018569e-01  5.83854079e-01]\n",
      "  [ 2.77730018e-01 -9.40572396e-02  8.77284884e-01 ... -8.00789297e-01\n",
      "   -1.50567651e-01  4.35288161e-01]\n",
      "  [ 2.92913169e-01  4.59451556e-01  1.08082390e+00 ... -8.12520683e-01\n",
      "   -2.80620456e-01  3.17230560e-02]\n",
      "  ...\n",
      "  [-3.54200393e-01  2.29425773e-01  7.44987309e-01 ... -1.03679508e-01\n",
      "   -2.60174066e-01  1.34258354e-02]\n",
      "  [-4.16917026e-01  4.15420309e-02  8.02270353e-01 ... -1.34316921e-01\n",
      "   -1.87624186e-01  1.06884927e-01]\n",
      "  [-4.90339339e-01  3.35844398e-01  7.81793892e-01 ... -1.80011958e-01\n",
      "   -1.80875748e-01 -9.65448283e-03]]\n",
      "\n",
      " [[-7.20090151e-01 -1.10861301e-01  3.37439865e-01 ... -1.00468241e-01\n",
      "    2.06924498e-01  7.44393647e-01]\n",
      "  [-8.84186774e-02 -2.29257658e-01  8.88929188e-01 ... -3.72530460e-01\n",
      "   -7.12003186e-02  4.31933343e-01]\n",
      "  [-5.37012279e-01  3.01813960e-01  1.07349384e+00 ... -2.14626536e-01\n",
      "    1.74486801e-01  2.32736960e-01]\n",
      "  ...\n",
      "  [-7.67670929e-01  5.66163138e-02  7.62813687e-01 ... -2.14397714e-01\n",
      "   -9.45551470e-02  7.49929249e-02]\n",
      "  [-4.46617126e-01 -2.57315207e-02  6.87849581e-01 ... -1.62010685e-01\n",
      "   -2.14856774e-01  1.11340091e-01]\n",
      "  [-4.52859163e-01 -5.38796261e-02  6.93691790e-01 ... -1.79055467e-01\n",
      "   -2.34291077e-01  1.49237648e-01]]\n",
      "\n",
      " [[-6.37734592e-01  3.83306891e-02  3.79163951e-01 ... -7.93580040e-02\n",
      "    3.31705868e-01  6.11161768e-01]\n",
      "  [ 2.83469856e-01 -1.75020471e-02  9.77913737e-01 ... -7.26521432e-01\n",
      "    4.96715494e-02  7.71547377e-01]\n",
      "  [ 8.18793327e-02  2.41281241e-01  7.27142870e-01 ... -8.33560467e-01\n",
      "   -3.34269941e-01 -7.55253360e-02]\n",
      "  ...\n",
      "  [-3.98912877e-01  1.49459885e-02  6.75557196e-01 ... -2.37472296e-01\n",
      "   -2.23030329e-01  2.78357357e-01]\n",
      "  [-4.76005375e-01  7.01260939e-02  6.57759845e-01 ... -2.15138346e-01\n",
      "   -2.27260798e-01  3.08191091e-01]\n",
      "  [-5.13955593e-01  1.07825279e-01  6.47597313e-01 ... -2.63436556e-01\n",
      "   -1.31048352e-01  2.77617842e-01]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-5.41577220e-01  8.09209421e-04  1.28474295e-01 ... -1.00590982e-01\n",
      "    1.65450305e-01  6.38524175e-01]\n",
      "  [-7.85839111e-02  6.81783706e-02  6.63811624e-01 ... -7.84071922e-01\n",
      "   -8.61209482e-02  7.11317599e-01]\n",
      "  [-2.84467071e-01 -6.09096363e-02  5.79471171e-01 ... -6.52161121e-01\n",
      "    3.88774835e-02  5.22078216e-01]\n",
      "  ...\n",
      "  [-3.06619883e-01  1.66979402e-01  5.30824423e-01 ... -3.48962486e-01\n",
      "   -3.54558170e-01  1.76538020e-01]\n",
      "  [-6.26712203e-01  3.95042002e-02  4.98382688e-01 ... -2.91209400e-01\n",
      "   -2.65012294e-01 -2.32718475e-02]\n",
      "  [-3.98303658e-01 -3.04171294e-02  5.05534232e-01 ... -2.23896593e-01\n",
      "   -2.06692860e-01  1.17199361e-01]]\n",
      "\n",
      " [[-6.61781132e-01  9.24918614e-03  3.08344126e-01 ... -8.18130672e-02\n",
      "    9.95036811e-02  6.23641431e-01]\n",
      "  [ 4.45644051e-01  1.70691147e-01  1.49684536e+00 ... -2.05872416e-01\n",
      "    2.59715140e-01  4.79300797e-01]\n",
      "  [ 1.74923569e-01 -1.22192517e-01  1.73207164e+00 ... -3.04264128e-02\n",
      "   -4.42668855e-01  9.89142299e-01]\n",
      "  ...\n",
      "  [-2.05970272e-01  8.10203105e-02  9.33477700e-01 ... -2.83104330e-01\n",
      "   -1.61712795e-01  7.45684355e-02]\n",
      "  [-2.20612526e-01  7.48499557e-02  9.55533743e-01 ... -2.49726817e-01\n",
      "   -2.08207786e-01  2.00500935e-01]\n",
      "  [-3.70125473e-01  1.84619009e-01  8.26677203e-01 ... -2.88528442e-01\n",
      "   -1.76722005e-01  1.36852115e-01]]\n",
      "\n",
      " [[-6.61858678e-01 -2.47629970e-01  4.33844090e-01 ... -8.58267173e-02\n",
      "    2.00889274e-01  7.36728489e-01]\n",
      "  [ 4.36522335e-01 -1.42216742e-01  1.53068984e+00 ... -1.02019534e-01\n",
      "    3.42808902e-01  3.98230880e-01]\n",
      "  [ 1.37114853e-01 -3.14916283e-01  1.81378484e+00 ...  1.53237998e-01\n",
      "   -5.10738552e-01  8.57322335e-01]\n",
      "  ...\n",
      "  [-2.65987694e-01  3.10766250e-02  8.21793139e-01 ... -2.08579957e-01\n",
      "   -1.40694931e-01 -4.12565432e-02]\n",
      "  [-3.75751019e-01  1.14415035e-01  8.30097914e-01 ... -2.11683542e-01\n",
      "   -1.63703650e-01  2.46153902e-02]\n",
      "  [-5.19618094e-01  1.43024087e-01  6.44480228e-01 ... -2.84470230e-01\n",
      "   -1.40085936e-01 -5.31397760e-02]]], shape=(1500, 38, 768), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reduce to 50 dim and use SVM to classify**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christian Gauthier\\AppData\\Local\\Temp\\ipykernel_26828\\218650055.py:12: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  X_train, X_test, y_train, y_test = train_test_split(reduced_outputs, df_slim['antigen.epitope'][0:1500], test_size=0.2, random_state=111)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.47333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#flatten the outputs to 2D\n",
    "flattened_outputs = tf.reshape(outputs, [outputs.shape[0], -1])\n",
    "pca = PCA(n_components=50)\n",
    "reduced_outputs = pca.fit_transform(flattened_outputs.numpy())  #tensor to numpy array before passing to PCA\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reduced_outputs, df_slim['antigen.epitope'][0:1500], test_size=0.2, random_state=111)\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THIS IS BASIC IMPLEMENTATION ABOVE**\n",
    "\n",
    "**BELOW IS FOLLOWING PAPER https://www.biorxiv.org/content/10.1101/2021.11.18.469186v1.full**\n",
    "\n",
    "Next step is to implement masked amino acid modelling \"hide, or “mask” 15% of the amino acids in each TCR amino acid sequence in the training set, and train TCR-BERT to predict these masked amino acids\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christian Gauthier\\AppData\\Local\\Temp\\ipykernel_26828\\1821716023.py:22: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  tokenized_inputs = tokenizer(df_slim['cdr3combined'][0:1500].tolist(), padding='max_length', truncation=True, max_length=38)\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "class TCRBertModel(tf.keras.Model):\n",
    "    def __init__(self, bert_model_name='bert-base-uncased', num_labels=None): \n",
    "        super(TCRBertModel, self).__init__()\n",
    "        self.bert = TFBertModel.from_pretrained(bert_model_name)\n",
    "        self.maa_head = tf.keras.layers.Dense(20, activation='softmax')  #MAA head for predicting masked amino acids 20 as thats how many amino acids there are\n",
    "        self.classification_head = tf.keras.layers.Dense(num_labels, activation='softmax') if num_labels else None  #classification head for downstream tasks\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        outputs = self.bert(inputs, training=training)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        maa_predictions = self.maa_head(sequence_output) #predict masked amino acids from output of BERT model 768 output to 20 amino acids\n",
    "        if training:\n",
    "            return maa_predictions\n",
    "        else:\n",
    "            classification_logits = self.classification_head(tf.reduce_mean(sequence_output, axis=1)) #takes the mean of the sequence output and passes it through the classification head \n",
    "            return maa_predictions, classification_logits\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenized_inputs = tokenizer(df_slim['cdr3combined'][0:1500].tolist(), padding='max_length', truncation=True, max_length=38)\n",
    "\n",
    "\n",
    "input_ids = tf.constant(tokenized_inputs['input_ids'])\n",
    "attention_mask = tf.constant(tokenized_inputs['attention_mask'])\n",
    "\n",
    "#inputs dictionary\n",
    "inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "\n",
    "model = TCRBertModel()\n",
    "maa_predictions = model(inputs, training=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0.01330471 0.03060793 0.03945042 ... 0.03930859 0.02659958 0.05257806]\n",
      "  [0.03285696 0.09712847 0.02395576 ... 0.05039798 0.0591586  0.0274514 ]\n",
      "  [0.04078352 0.0518089  0.06029619 ... 0.04855753 0.01902218 0.01815997]\n",
      "  ...\n",
      "  [0.02975798 0.05448749 0.05213316 ... 0.03754457 0.03323095 0.03513494]\n",
      "  [0.02713712 0.07107564 0.05515148 ... 0.03899817 0.03625135 0.03669605]\n",
      "  [0.03008739 0.06630372 0.04762534 ... 0.03806898 0.03547477 0.04136902]]\n",
      "\n",
      " [[0.0197041  0.03878022 0.05492344 ... 0.0444821  0.04129329 0.10465682]\n",
      "  [0.03690844 0.04299334 0.03430608 ... 0.03004827 0.04652827 0.02210557]\n",
      "  [0.08066549 0.0574407  0.0546074  ... 0.04275035 0.0210504  0.02547678]\n",
      "  ...\n",
      "  [0.03700403 0.05686714 0.02856055 ... 0.02641634 0.02939809 0.050872  ]\n",
      "  [0.03169116 0.05300458 0.03902589 ... 0.03016533 0.0240878  0.03085233]\n",
      "  [0.03559051 0.05108725 0.04177984 ... 0.02913353 0.02373109 0.03068066]]\n",
      "\n",
      " [[0.01923585 0.05690232 0.04598095 ... 0.02798671 0.03595887 0.04879451]\n",
      "  [0.026465   0.07981341 0.01818522 ... 0.04555115 0.06204415 0.01603462]\n",
      "  [0.06492686 0.06385954 0.02876687 ... 0.02590829 0.03370367 0.0388713 ]\n",
      "  ...\n",
      "  [0.02675746 0.06505572 0.03129879 ... 0.03957994 0.03098122 0.02983482]\n",
      "  [0.02638719 0.0753699  0.02833609 ... 0.04587822 0.02987503 0.03067017]\n",
      "  [0.02607524 0.07700662 0.02755581 ... 0.0444141  0.02529102 0.02943585]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.020097   0.03560458 0.06546243 ... 0.03995156 0.03382535 0.05805227]\n",
      "  [0.05066504 0.08584137 0.03497126 ... 0.033401   0.04041267 0.01877764]\n",
      "  [0.04218496 0.03045856 0.02948265 ... 0.02178609 0.00694646 0.05827047]\n",
      "  ...\n",
      "  [0.02843824 0.06782085 0.0330837  ... 0.03290163 0.02406935 0.03952028]\n",
      "  [0.03804765 0.06793401 0.04629199 ... 0.02935371 0.02995286 0.03732571]\n",
      "  [0.03438642 0.06927159 0.0421661  ... 0.02797336 0.02948791 0.03812199]]\n",
      "\n",
      " [[0.01044328 0.04011013 0.0674748  ... 0.08178805 0.02161009 0.03006622]\n",
      "  [0.01784014 0.10292163 0.05401766 ... 0.06439026 0.03063892 0.01419515]\n",
      "  [0.01868557 0.04410314 0.07526878 ... 0.07191221 0.04086323 0.0181832 ]\n",
      "  ...\n",
      "  [0.02957984 0.05907325 0.04043704 ... 0.02777268 0.02275567 0.03762337]\n",
      "  [0.01863101 0.07146747 0.04579222 ... 0.03572943 0.02472423 0.02613334]\n",
      "  [0.02064542 0.07853784 0.04759562 ... 0.03531911 0.02764324 0.02843229]]\n",
      "\n",
      " [[0.00835721 0.04144677 0.08273003 ... 0.07361169 0.03115591 0.03578994]\n",
      "  [0.01380279 0.1383324  0.05944071 ... 0.05112831 0.04688474 0.01471587]\n",
      "  [0.02130526 0.05045332 0.1306995  ... 0.06492505 0.0352284  0.01870786]\n",
      "  ...\n",
      "  [0.01805691 0.0784616  0.04753973 ... 0.03031659 0.02840223 0.03516586]\n",
      "  [0.01852426 0.07385414 0.05682906 ... 0.0253745  0.02634928 0.02867004]\n",
      "  [0.02347325 0.0873655  0.04914907 ... 0.02427446 0.02615089 0.029004  ]]], shape=(1500, 38, 20), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(maa_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christian Gauthier\\AppData\\Local\\Temp\\ipykernel_26828\\521817857.py:7: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  labels = df_slim['antigen.epitope'][0:1500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.31\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=50)  \n",
    "\n",
    "maa_predictions_np = maa_predictions.numpy()  \n",
    "flattened_maa_predictions_np = maa_predictions_np.reshape(maa_predictions_np.shape[0], -1)\n",
    "pca_outputs = pca.fit_transform(flattened_maa_predictions_np)\n",
    "\n",
    "labels = df_slim['antigen.epitope'][0:1500]\n",
    "X_train, X_test, y_train, y_test = train_test_split(pca_outputs, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "svm_classifier = SVC(kernel='rbf', C=1.0)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Somehow worse*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christian Gauthier\\AppData\\Local\\Temp\\ipykernel_26828\\2874704034.py:23: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  tokenized_inputs = tokenizer(df_slim['cdr3combined'][0:1500].tolist(), padding='max_length', truncation=True, max_length=38)\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_13/bert/pooler/dense/kernel:0', 'tf_bert_model_13/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_13/bert/pooler/dense/kernel:0', 'tf_bert_model_13/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "12/12 [==============================] - 124s 9s/step - loss: 2.9405\n",
      "Epoch 2/3\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.8754\n",
      "Epoch 3/3\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.8690\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20154f93910>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "class TCRBertModel(tf.keras.Model):\n",
    "    def __init__(self, bert_model_name='bert-base-uncased', num_labels=None): \n",
    "        super(TCRBertModel, self).__init__()\n",
    "        self.bert = TFBertModel.from_pretrained(bert_model_name)\n",
    "        self.maa_head = tf.keras.layers.Dense(20, activation='softmax')  #MAA head for predicting masked amino acids\n",
    "        self.classification_head = tf.keras.layers.Dense(num_labels, activation='softmax') if num_labels else None  #classification head for downstream tasks\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        outputs = self.bert(inputs, training=training)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        maa_predictions = self.maa_head(sequence_output) #predict masked amino acids from the output of the BERT model\n",
    "        if training:\n",
    "            return maa_predictions\n",
    "        else:\n",
    "            classification_logits = self.classification_head(tf.reduce_mean(sequence_output, axis=1)) #take the mean of the sequence output and pass it through the classification head \n",
    "            return maa_predictions, classification_logits\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenized_inputs = tokenizer(df_slim['cdr3combined'][0:1500].tolist(), padding='max_length', truncation=True, max_length=38)\n",
    "\n",
    "#convert the tokenized sequences to tensors\n",
    "input_ids = tf.constant(tokenized_inputs['input_ids'])\n",
    "attention_mask = tf.constant(tokenized_inputs['attention_mask'])\n",
    "\n",
    "\n",
    "inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "model = TCRBertModel()\n",
    "\n",
    "#MAA pre-training objective\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "model.fit(inputs, maa_predictions, epochs=3, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "maa_predictions = model(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christian Gauthier\\AppData\\Local\\Temp\\ipykernel_26828\\521817857.py:7: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  labels = df_slim['antigen.epitope'][0:1500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2966666666666667\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=50)  \n",
    "\n",
    "maa_predictions_np = maa_predictions.numpy()  \n",
    "flattened_maa_predictions_np = maa_predictions_np.reshape(maa_predictions_np.shape[0], -1)\n",
    "pca_outputs = pca.fit_transform(flattened_maa_predictions_np)\n",
    "\n",
    "labels = df_slim['antigen.epitope'][0:1500]\n",
    "X_train, X_test, y_train, y_test = train_test_split(pca_outputs, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "svm_classifier = SVC(kernel='rbf', C=1.0)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
